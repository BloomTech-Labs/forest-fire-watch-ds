{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is a training run to save two deployable models with pipelines: KNN and RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# X_train = pd.read_pickle('/home/nedderlander/datascience/burn notice/Data-Science/Data/X_train_full.pkl')\n",
    "# y_train = pd.read_pickle('/home/nedderlander/datascience/burn notice/Data-Science/Data/y_train_full.pkl')\n",
    "X_test = pd.read_pickle('/home/nedderlander/datascience/burn notice/Data-Science/Data/X_test_full.pkl')\n",
    "# y_test = pd.read_pickle('/home/nedderlander/datascience/burn notice/Data-Science/Data/y_test_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latitude',\n",
       " 'longitude',\n",
       " 'brightness',\n",
       " 'scan',\n",
       " 'track',\n",
       " 'acq_time',\n",
       " 'satellite',\n",
       " 'confidence',\n",
       " 'bright_t31',\n",
       " 'frp',\n",
       " 'daynight',\n",
       " 'type',\n",
       " 'FIRE_YEAR',\n",
       " 'MONTH',\n",
       " 'WEEK',\n",
       " 'DAY']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    163798\n",
       "2      7400\n",
       "3        60\n",
       "1        50\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# feature processing\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# pre-processing pipeline\n",
    "column_trans = ColumnTransformer(\n",
    "    [('onehot', ce.OneHotEncoder(), ['satellite', 'daynight', 'type', 'FIRE_YEAR', 'MONTH']),\n",
    "     ('scale', StandardScaler(), ['brightness', 'track', 'scan', 'acq_time', 'confidence', 'bright_t31', 'frp'])],\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "random_state = 314\n",
    "\n",
    "pipelines ={ \n",
    "    'rfc' : make_pipeline(column_trans, RandomForestClassifier(random_state = random_state )),\n",
    "    'knn' : make_pipeline(column_trans, KNeighborsClassifier()),\n",
    "}\n",
    "\n",
    "# create a class weights grid:\n",
    "\n",
    "class_weights = [\n",
    "    {0:1, 1:1},\n",
    "    {0:1, 1:2},\n",
    "    {0:1, 1:5},\n",
    "    {0:1, 1:10},\n",
    "    {0:1, 1:100},\n",
    "    {0:1, 1:1000}\n",
    "]\n",
    "\n",
    "# Create a hyperparameter grid for Random Forest\n",
    "\n",
    "rfc_hyperparameters = { \n",
    "    'randomforestclassifier__n_estimators' : [100, 200] ,\n",
    "    'randomforestclassifier__max_features' : ['auto', 0.3, 0.6],\n",
    "    'randomforestclassifier__class_weight' : class_weights\n",
    "}\n",
    "\n",
    "knn_hyperparameters = {\n",
    "    'kneighborsclassifier__n_neighbors' : [3, 5, 10, 20],\n",
    "    'kneighborsclassifier__weights' : ['uniform', 'distance'],\n",
    "    'kneighborsclassifier__algorithm' : ['ball_tree', 'kd_tree'],\n",
    "    'kneighborsclassifier__leaf_size' : [15, 30, 45, 75]\n",
    "}\n",
    "\n",
    "# Create the hyperparameter_grids dictionary\n",
    "\n",
    "hyperparameter_grids = {\n",
    "    'rfc' : rfc_hyperparameters,\n",
    "    'knn' : knn_hyperparameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfc was found, and it is a dict\n",
      "knn was found, and it is a dict\n"
     ]
    }
   ],
   "source": [
    "#check to see if our dictionaries match\n",
    "\n",
    "\n",
    "for key in pipelines.keys():\n",
    "    if key in hyperparameter_grids:\n",
    "        if type(hyperparameter_grids[key]) is dict:\n",
    "            print(key, 'was found, and it is a dict')\n",
    "        else:\n",
    "            print(key, 'was found, and it is not a dict')\n",
    "    else:\n",
    "        print(key, 'was not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['rfc', 'knn'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "\n",
    "models = {}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for key in pipelines.keys():\n",
    "    models[key] = RandomizedSearchCV(\n",
    "        pipelines[key],\n",
    "        hyperparameter_grids[key],\n",
    "        scoring='f1',\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "        verbose=10\n",
    "  )\n",
    "\n",
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 46.3min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 63.6min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 84.4min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 106.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfc is trained\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 75.1min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 155.1min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 164.4min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 206.0min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 223.4min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 267.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn is trained\n"
     ]
    }
   ],
   "source": [
    "#Run cross validation on the models\n",
    "\n",
    "for key in models.keys():\n",
    "    models[key].fit(X_train, y_train)\n",
    "    print(key, 'is trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# save models and test results\n",
    "for key, item in models.items():\n",
    "    \n",
    "    joblib_file ='{}_trained_full.pkl'.format(key)\n",
    "    model = item.best_estimator_\n",
    "    \n",
    "    joblib.dump(model, joblib_file)\n",
    "    \n",
    "    results = pd.DataFrame(item.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "    joblib_file = '{}_training_results.pkl'.format(key)\n",
    "    joblib.dump(results, joblib_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is code to check outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to reload and examine\n",
    "\n",
    "rfc = joblib.load('rfc_trained_full.pkl')\n",
    "\n",
    "rfc_results = joblib.load('rfc_training_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
